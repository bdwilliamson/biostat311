---
title: 'BIOST 311: Assumptions and diagnostics for linear regression'
author: "Arjun Sondhi"
date: "4/25/2018"
fontsize: 12pt
output: 
  beamer_presentation:
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

## Linear regression model

Recall the form of the (multiple) linear regression model:

$$
\begin{aligned}
E[Y_i \mid X_{i1}, \dots, X_{ip}] &= \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} \\
i &= 1, ..., n
\end{aligned}
$$

This is equivalent to:

$$
\begin{aligned}
Y_i &= \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \epsilon_i \\
\epsilon_i &\sim \text{zero mean} \\
i &= 1, ..., n
\end{aligned}
$$

\pause

\textbf{Inference:} understand the relationship between $Y$ and the $X$s; how does variable $Y$ change as some variables $X$ change? 

## Outline

In this lecture, we'll discuss:

- Assumptions necessary for proper inference with linear regression
- Diagnostic tools to evaluate assumptions 

## Linear regression assumptions

$$
\begin{aligned}
Y_i &= \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \epsilon_i \\
i &= 1, ..., n
\end{aligned}
$$

The following are considered "classical" assumptions for linear regression:

- A1) $E[Y_i | X_i] = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}$
- A2) $\epsilon_i$ are independent of $X_i$
- A3) $\epsilon_i$ are independently distributed as $N(0, \sigma^2)$
- A4) The predictor matrix $X$ is full rank

## A1) Correctly specified mean model

\textbf{What does this mean?}

The true population conditional mean $Y|X$ is a linear function of the predictors. 

\pause

\textbf{What happens if we violate this assumption?}

It depends!

## A1) Correctly specified mean model

There are many ways to misspecify the mean model:

- Not inlcuding a variable associated with the response
- Not including a confounding variable  
- Specifying an incorrect relationship (e.g. including only a variable's linear term when it's quadratically associated)
- ...

Any of these might lead to varying degrees of biased estimation and inaccurate inference (or not!). 

## A1) Correctly specified mean model

\textbf{What can we do about it?}

- Think carefully about which variables are scientifically relevant
- Specify more flexible relationships (splines, higher-order terms, interactions), but be careful about overfitting!
- Model diagnostics

## A2) $\epsilon_i$ are independent of $X_i$

\textbf{What does this mean?}

This assumption requires that the variation in the errors cannot be explained by the predictors. 

How might this situation arise? 

\pause

Suppose $Y = \beta_0 + \beta_1X + \beta_2Z + \tilde{\epsilon}$, where $X$ and $Z$ are (mildly) correlated. Also, suppose we do not observe $Z$, and fit a regression with an intercept and $X$ only.

\pause

Then, our model is $Y = \beta_0 + \beta_1X + \epsilon$ where $\epsilon = \beta_2Z + \tilde{\epsilon}$. Through the correlation of $X$ and $Z$, we can see that $X$ and $\epsilon$ are not independent. 

## A2) $\epsilon_i$ are independent of $X_i$

```{r, size="tiny"}
# Generate data
z = rnorm(1000)
x = 0.5*rnorm(1000) + 0.5*z # Z is a confounder for X
y = 0.3 + 2*x + z + rnorm(1000)
m = lm(y ~ x + z) # Fit linear regression model
summary(m)
confint(m)
```

## A2) $\epsilon_i$ are independent of $X_i$

```{r, size="tiny"}
# Generate data
z = rnorm(1000)
x = 0.5*rnorm(1000) + 0.5*z # Z is a confounder for X
y = 0.3 + 2*x + z + rnorm(1000)
m = lm(y ~ x) # Omitted Z
summary(m)
confint(m)
```

## A2) $\epsilon_i$ are independent of $X_i$

```{r, size="tiny"}
# Generate data
z = rnorm(1000)
x = rnorm(1000) # Z independent of X
y = 0.3 + 2*x + z + rnorm(1000)
m = lm(y ~ x) # Omitted Z
summary(m)
confint(m)
```


## A2) $\epsilon_i$ are independent of $X_i$

\textbf{What happens if we violate this assumption?}

Violation of this assumption can result in biased estimation of $\beta$ coefficients and inaccurate inference. 

\textbf{What can we do about it?}

A serious violation of this assumption could be captured via a residuals vs. fitted values plot, which we'll see later.


## A3) $\epsilon_i$ are independently distributed as $N(0, \sigma^2)$

\textbf{What does this mean?}

There are three major components here: errors are \textit{independent}, errors are \textit{normally distributed}, and errors have a \textit{constant variance}.  


## A3a) Independent errors

\textbf{What does this mean?}

The observations are randomly sampled such that there is no possible dependence between their responses. An example of dependent errors would be a longitudinal study where subjects contribute multiple measurements over time.

\pause

\textbf{What happens if we violate this assumption?}

Both model-based and robust standard errors will give incorrect inference. 

\textbf{What can we do about it?}

Advanced methods for analyzing dependent data. Talk to Kelsey if you want to hear more!!!

## A3b) Normal errors

\textbf{What does this mean?}

The errors are normally distributed.

\pause

\textbf{What happens if we violate this assumption?}

If we have normal errors (and all other assumptions hold), then the regression estimator $\hat{\beta}$ is also the maximum likelihood estimator, which means it is the asymptotically \alert{optimal unbiased} estimator. In other words, it is the unbiased estimator with the \alert{lowest variance}. 

Without normality, we still have that $\hat{\beta}$ is the unbiased estimator with the lowest variance among all other \alert{linear} estimators.

In other words, without normality, we still have pretty good estimators; there may simply be better nonlinear estimators.


## A3b) Normal errors

\textbf{What can we do about it?}

Transforming $Y$ can help make errors look more normal. However, if the transformation is not scientifically meaningful, it's probably not worth it. 

In general, we care mostly about having unbiased estimation; low variance is a secondary concern. 


## A3c) Constant variance (homoskedasticity) errors

\textbf{What does this mean?}

Among all possible levels of $X$, the variance of $Y$ is the same. In math, $Var(Y|X=x) = \sigma^2$ for all possible $x \in R^p$.

\pause

\centering
\includegraphics{hetero.png}

## A3c) Constant variance (homoskedasticity) errors

\textbf{What happens if we violate this assumption?}

If we use model-based standard errors, then these will give incorrect inference (i.e. confidence intervals may not properly cover the true $\beta$). 

If we use robust standard errors, then we're okay! 

\textbf{What can we do about it?}

Use robust standard errors! (\texttt{regress} uses these by default while \texttt{lm} does not!)



## A4) Full rank predictor matrix

\textbf{What does this mean?}

A matrix $X$ is "full rank" if none of its columns are \alert{collinear}.

In other words, we cannot have two columns $X_1$ and $X_2$ which can be written as $X_1 = aX_2 + b$ (e.g. age in months and age in years). Why might this be an issue, intuitively?

\pause

\textbf{What happens if we violate this assumption?}

Let's see what happens if we try to fit a model with collinear predictors.

## A4) Full rank predictor matrix

```{r rank, size="tiny"}
# Generate data
x1 = rnorm(100)
x2 = 2*x1 + 3
y = 2*x1 - x2 + rnorm(100)
# Fit linear regression model
m = lm(y ~ x1 + x2)
summary(m)
```

## A4) Full rank predictor matrix

We cannot fit regression coefficients for all collinear variables! 

If variables are strongly, but not perfectly correlated, coefficients can still be fit. However, stronger correlation will lead to larger standard errors, adding more uncertainty to inference.

\pause
\textbf{What can we do about it?}

Including very strongly correlated variables doesn't really make sense, as we are repeating information. It's better to screen out variables which are not really necessary.

Fortunately, we can easily look at pairwise correlations in our observed data.

## A4) Full rank predictor matrix

Another situation that results in a non-full rank $X$ matrix is when the number of variables, $p > n$, the number of observations (\alert{high-dimensional}). 

In this situation, ordinary linear regression models cannot be fit; we need to add more constraints to the model in order to obtain parameter estimates.

If interested in this area, talk to Brian!!!

## Recap

What do we \textit{actually} need?

\pause

- Independent observations
- Robust standard errors
- $n$ sufficiently large; in particular, $n >> p$
- Predictors are not strongly correlated
- A model specification that is \textit{close enough} to the ground truth

## Diagnostics

Can we test if our model assumptions are met? 

\pause

Kind of. Diagnostics are a sanity check to see if your model is \alert{terrible} or not. 

\pause

What diagnostics do NOT do:

- Identify all possible issues in a model
- Provide reliable \alert{model selection} criteria

## Residuals

After fitting a regression model, the \alert{residuals} for each observation are defined as $r_i = y_i - \sum_{i=0}^p \hat{\beta}_i x_i$
\centering
\includegraphics{linear-regr-ls.jpg}


## Residual plots

Why do we consider the regression residuals for diagnostics? 

\pause

The residuals are our "best guess" for the distribution of the $\epsilon_i$, which a lot of the regression assumptions involve.

\pause

The most common plot for linear regression diagnostics is a scatterplot of residuals vs. \alert{fitted values} (defined as $\sum_{i=0}^p \hat{\beta}_i x_i$). 

## Residuals vs. fitted values

What we look for in this plot:

- Residuals spread randomly around the $y = 0$ --> $\epsilon$ zero mean
- No "trend" in the residuals --> $\epsilon$ independent of $X$
- Constant height between min and max residuals at each level of fitted values --> constant variance

## Residuals vs. fitted values

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(100)
x2 = c(rep(1, 50), rep(2,50))
epsilon = rnorm(100) # constant error variance
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
plot(fitted(m), residuals(m), xlab = "Fitted values", ylab = "Residuals", pch=19)
abline(h=0)
```


## Residuals vs. fitted values

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(100)
x2 = c(rep(1, 50), rep(2,50))
epsilon = c(rnorm(50), rnorm(50, sd = 0.1)) # non-constant error variance
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
plot(fitted(m), residuals(m), xlab = "Fitted values", ylab = "Residuals", pch=19, col = x2)
abline(h=0)
```


## Residuals vs. fitted values

```{r, size="tiny",fig.height=5,fig.width=8}
# Generate data
x1 = rnorm(100)
x2 = c(rep(1, 50), rep(2,50))
epsilon = rnorm(100) 
y = 3 + exp(2*x1) + 1.5*x2 + epsilon
# Fit linear regression with misspeccified model
m = lm(y ~ x1)
plot(fitted(m), residuals(m), xlab = "Fitted values", ylab = "Residuals", pch=19)
abline(h=0)
```


## Residual Q-Q plot

A Q-Q (quantile-quantile) plot compares two distributions by plotting their quantiles against each other. The distributions are equal if the resulting scatterplot goes through the 45-degree diagonal line

We use Normal Q-Q plots of residuals to judge if the model errors are normally distributed.

## Residual Q-Q plot

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(1000)
x2 = c(rep(1, 500), rep(2,500))
epsilon = rnorm(1000)
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
qqnorm(residuals(m)) # QQ-plot comparing residuals to N(0,1) distribution
abline(0,1)
```


## Residual Q-Q plot

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(1000)
x2 = c(rep(1, 500), rep(2,500))
epsilon = rnorm(1000, sd = 3) # Still normal, but no longer with variance = 1
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
qqnorm(residuals(m)) # QQ-plot comparing residuals to N(0,1) distribution
abline(0,1)
```

## Residual Q-Q plot

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(1000)
x2 = c(rep(1, 500), rep(2,500))
epsilon = rnorm(1000, sd = 3) # Still normal, but no longer with variance = 1
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
qqnorm(stdres(m)) # QQ-plot comparing *standardized* residuals to N(0,1) distribution
abline(0,1)
```

## Residual ACF plot

The ACF (autocorrelation function) plot displays the empirical correlation of all observations separated by various time lags. For randomly sampled data with no natural ordering of the observations, we would expect these correlations to be small and non-significant. 

We can look at the autocorrelation of the residuals to determine if the independent $\epsilon$ assumption holds. This might be a good idea if we are less aware of how the data were generated, and there is the possiblity of correlated observations. 

## Residual ACF plot

```{r, size="tiny",fig.height=5,fig.width=8}
x1 = rnorm(1000)
x2 = c(rep(1, 500), rep(2,500))
epsilon = rnorm(1000)
y = 3 + 0.3*x1 + 1.5*x2 + epsilon
m = lm(y ~ x1 + x2)
acf(resid(m), lag.max=10) 
```

## Summary of diagnostics

- By examining a residuals vs. fitted values plot, we can determine if we have a serious model misspecification issue (if we just have non-constant variance, this is less of a big deal with large enough $n$)
- A Q-Q plot of the standardized residuals can tell us if the normality of residuals assumption is justifiable (but this is also less of a big deal with large enough $n$)
- The residual ACF plot can be useful if we think there is a possiblity of correlated data, in which case standard linear regression would not work out


